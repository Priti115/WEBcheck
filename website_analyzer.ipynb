{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdeef88-b6ad-4529-ba8d-580e66ca322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff15b70c-5533-4f79-b2ee-0991ad5c6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_seo_data(url):\n",
    "    \"\"\"\n",
    "    Extracts SEO-related data from a given website URL.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The website URL to analyze.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the title, meta description, H1 tags, canonical URL, and robots meta tag.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return {\"error\": f\"Failed to fetch page, status code: {response.status_code}\"}\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        seo_data = {\n",
    "            \"Title\": soup.title.string if soup.title else \"No Title\",\n",
    "            \"Meta Description\": (\n",
    "                soup.find(\"meta\", attrs={\"name\": \"description\"})[\"content\"]\n",
    "                if soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                else \"No Meta Description\"\n",
    "            ),\n",
    "            \"H1 Tags\": [h1.text.strip() for h1 in soup.find_all(\"h1\")],\n",
    "            \"Canonical URL\": (\n",
    "                soup.find(\"link\", attrs={\"rel\": \"canonical\"})[\"href\"]\n",
    "                if soup.find(\"link\", attrs={\"rel\": \"canonical\"})\n",
    "                else \"No Canonical URL\"\n",
    "            ),\n",
    "            \"Robots Meta\": (\n",
    "                soup.find(\"meta\", attrs={\"name\": \"robots\"})[\"content\"]\n",
    "                if soup.find(\"meta\", attrs={\"name\": \"robots\"})\n",
    "                else \"No Robots Tag\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        return seo_data\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7064797-ef22-402d-b256-6d7e605a2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_url(url):\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"https://\" + url  # Default to 'https://' for security\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16de9818-224c-4f7c-b0a3-5fd139d03b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_website_type(url, seo_data):\n",
    "    \"\"\"\n",
    "    Determines whether a website is a personal portfolio, company/business site, or uncategorized.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL.\n",
    "        seo_data (dict): Extracted SEO data containing Title and Meta Description.\n",
    "\n",
    "    Returns:\n",
    "        str: The detected website type - 'Personal Portfolio/Project', 'Company/Business Website', or 'Uncategorized Website'.\n",
    "    \"\"\"\n",
    "\n",
    "    personal_domains = [\"github.io\", \"vercel.app\", \"netlify.app\", \".me\", \".dev\"]\n",
    "    company_keywords = [\"products\", \"services\", \"pricing\", \"customers\", \"company\",\n",
    "                        \"enterprise\", \"platform\", \"official\", \"cloud\", \"ai\", \"business\"]\n",
    "    portfolio_keywords = [\"portfolio\", \"projects\", \"about me\", \"skills\", \"resume\"]\n",
    "    known_companies = [\"kaggle.com\", \"flipkart.com\", \"amazon.com\", \"google.com\"]\n",
    "\n",
    "    # Convert SEO text to lowercase for case-insensitive comparison\n",
    "    meta_desc = seo_data.get(\"Meta Description\", \"\").lower()\n",
    "    title = seo_data.get(\"Title\", \"\").lower()\n",
    "\n",
    "    # Check if it's a personal portfolio or project\n",
    "    is_personal = any(domain in url for domain in personal_domains) or \\\n",
    "                  any(keyword in meta_desc for keyword in portfolio_keywords)\n",
    "\n",
    "    # Check if it's a company/business website\n",
    "    is_company = any(keyword in meta_desc for keyword in company_keywords) or \\\n",
    "                 any(keyword in title for keyword in company_keywords) or \\\n",
    "                 any(comp in url for comp in known_companies)\n",
    "\n",
    "    if is_personal:\n",
    "        return \"Personal Portfolio/Project\"\n",
    "    elif is_company:\n",
    "        return \"Company/Business Website\"\n",
    "    else:\n",
    "        return \"Uncategorized Website\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6a28d9-1767-48cf-8a95-839c63e29f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_portfolio(seo_data):\n",
    "    \"\"\"\n",
    "    Analyzes a portfolio website based on SEO data and assigns a portfolio relevance score.\n",
    "\n",
    "    Parameters:\n",
    "        seo_data (dict): Extracted SEO data containing Title, Meta Description, and H1 Tags.\n",
    "\n",
    "    Returns:\n",
    "        float: Portfolio score as a percentage (0-100).\n",
    "    \"\"\"\n",
    "    portfolio_score = 0\n",
    "    total_checks = 4  \n",
    "\n",
    "    # Extract necessary data\n",
    "    meta_desc = seo_data.get(\"Meta Description\", \"\").lower()\n",
    "    h1_tags = \" \".join(seo_data.get(\"H1 Tags\", [])).lower()  \n",
    "\n",
    "    # Check for relevant keywords in Meta Description and H1 Tags\n",
    "    keywords = [\"skills\", \"projects\", \"contact\"]\n",
    "    \n",
    "    if seo_data.get(\"H1 Tags\"):\n",
    "        portfolio_score += 1  # At least one H1 tag exists\n",
    "\n",
    "    for keyword in keywords:\n",
    "        if keyword in meta_desc or keyword in h1_tags:\n",
    "            portfolio_score += 1\n",
    "\n",
    "    # Convert score to percentage\n",
    "    portfolio_score = (portfolio_score / total_checks) * 100  \n",
    "    return round(portfolio_score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ab59b3-646e-46c1-9ee5-cd7ba60814f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seo_score(seo_data):\n",
    "    \"\"\"\n",
    "    Calculates an SEO score for company websites based on key SEO elements.\n",
    "\n",
    "    Parameters:\n",
    "        seo_data (dict): Extracted SEO data containing Title, Meta Description, H1 Tags, Canonical URL, and Robots Meta.\n",
    "\n",
    "    Returns:\n",
    "        float: SEO score as a percentage (0-100).\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    total_checks = 5  \n",
    "\n",
    "    # Extract necessary data\n",
    "    title = seo_data.get(\"Title\", \"No Title\")\n",
    "    meta_desc = seo_data.get(\"Meta Description\", \"No Meta Description\")\n",
    "    h1_tags = seo_data.get(\"H1 Tags\", [])\n",
    "    canonical = seo_data.get(\"Canonical URL\", \"No Canonical URL\")\n",
    "    robots_meta = seo_data.get(\"Robots Meta\", \"\")\n",
    "\n",
    "    # Scoring criteria\n",
    "    if title and title != \"No Title\":\n",
    "        score += 1\n",
    "    \n",
    "    if meta_desc and meta_desc != \"No Meta Description\":\n",
    "        score += 1\n",
    "    \n",
    "    if h1_tags:\n",
    "        score += 1\n",
    "    \n",
    "    if canonical and canonical != \"No Canonical URL\":\n",
    "        score += 1\n",
    "    \n",
    "    if robots_meta and \"index\" in robots_meta.lower():\n",
    "        score += 1\n",
    "\n",
    "    # Convert score to percentage\n",
    "    seo_score = (score / total_checks) * 100  \n",
    "    return round(seo_score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf638ec-9eb1-4288-aff9-26c079183f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_website(url):\n",
    "    \"\"\"\n",
    "    Detects the type of website (Personal Portfolio, Company/Business, or Uncategorized)\n",
    "    and performs the appropriate analysis.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the website type, portfolio score (if applicable),\n",
    "              or SEO score (if applicable).\n",
    "    \"\"\"\n",
    "    url = fix_url(url)  # Ensure URL format is correct\n",
    "    # print(f\"üîç Checking URL: {url}\")\n",
    "\n",
    "    seo_data = fetch_seo_data(url)\n",
    "\n",
    "    if \"error\" in seo_data:\n",
    "        print(\"‚ùå Error fetching data:\", seo_data[\"error\"])\n",
    "        return {\"error\": seo_data[\"error\"]}\n",
    "\n",
    "    website_type = detect_website_type(url, seo_data)\n",
    "    print(f\"üåç Website Type Detected: {website_type}\")\n",
    "\n",
    "    result = {\"Website Type\": website_type}\n",
    "\n",
    "    if website_type == \"Personal Portfolio/Project\":\n",
    "        portfolio_score = analyze_portfolio(seo_data)\n",
    "        result[\"Portfolio Score\"] = portfolio_score\n",
    "        # print(f\"üìù Portfolio Score: {portfolio_score}%\")\n",
    "\n",
    "    elif website_type == \"Company/Business Website\":\n",
    "        seo_score = calculate_seo_score(seo_data)\n",
    "        result[\"SEO Score\"] = seo_score\n",
    "        # print(f\"üìä SEO Score: {seo_score}%\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è This website is Uncategorized. Cannot analyze.\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767e881c-9cae-4565-a1a0-b5b4978153be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da82c78c-cde5-4a96-828f-ce61dbf2e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ChromeDriver ka path\n",
    "chrome_driver_path = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# ‚úÖ WebDriver Options (Bot Detection Bypass)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    ")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "def analyze_ui(url):\n",
    "    \"\"\"\n",
    "    Analyzes the UI of a given website by extracting the number of images, buttons, and links.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing page title, number of UI elements, and UI score.\n",
    "    \"\"\"\n",
    "    url = fix_url(url)  # Ensure URL is formatted correctly\n",
    "    # print(f\"\\nüöÄ Testing UI: {url}\")\n",
    "\n",
    "    # ‚úÖ Start WebDriver\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # ‚úÖ Wait for JavaScript elements to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # ‚úÖ Extract UI Elements\n",
    "    images = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "    buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "    # ‚úÖ UI Score Calculation (Normalized)\n",
    "    total_elements = len(images) + len(buttons) + len(links)\n",
    "    max_elements = 100  # Normalizing factor\n",
    "    ui_score = min(round((total_elements / max_elements) * 100, 2), 100)  # Capped at 100%\n",
    "\n",
    "    # ‚úÖ Prepare Output\n",
    "    result = {\n",
    "        \"Page Title\": driver.title,\n",
    "        # \"Images\": len(images),\n",
    "        # \"Buttons\": len(buttons),\n",
    "        # \"Links\": len(links),\n",
    "        \"UI Score\": ui_score,\n",
    "    }\n",
    "\n",
    "    # ‚úÖ Print Results\n",
    "    # print(f\"üîç Page Title: {result['Page Title']}\")\n",
    "    # print(f\"üñºÔ∏è Images: {result['Images']}, üîò Buttons: {result['Buttons']}, üîó Links: {result['Links']}\")\n",
    "    # print(f\"üé® UI Score: {result['UI Score']}%\")\n",
    "\n",
    "    # ‚úÖ Close WebDriver\n",
    "    driver.quit()\n",
    "    # print(f\"‚úÖ Finished Testing: {url}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42712eb0-9ec4-471b-96e8-5bbe22e962f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ChromeDriver ka exact path specify karo\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "def analyze_ux(url):\n",
    "    \"\"\"\n",
    "    Analyzes the UX of a given website by extracting key interaction elements.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the page title, number of UI elements, and UX score.\n",
    "    \"\"\"\n",
    "    url = fix_url(url)  # Ensure correct URL format\n",
    "    # print(f\"\\nüöÄ Testing UX: {url}\")\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in the background\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "    # ‚úÖ Start WebDriver with proper path\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        # ‚úÖ Open the website\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # ‚úÖ Extract Page Title\n",
    "        page_title = driver.title\n",
    "        # print(f\"üîç Page Title: {page_title}\")\n",
    "\n",
    "        # ‚úÖ Count UX-related elements\n",
    "        total_images = len(driver.find_elements(By.TAG_NAME, \"img\"))\n",
    "        total_buttons = len(driver.find_elements(By.TAG_NAME, \"button\"))\n",
    "        total_links = len(driver.find_elements(By.TAG_NAME, \"a\"))\n",
    "\n",
    "        # print(f\"üñºÔ∏è Images: {total_images}, üîò Buttons: {total_buttons}, üîó Links: {total_links}\")\n",
    "\n",
    "        # ‚úÖ UX Score Calculation (Normalized)\n",
    "        max_elements = 100  # Normalization factor\n",
    "        ux_score = min(round(((total_images + total_buttons + total_links) / max_elements) * 100, 2), 100)\n",
    "\n",
    "        # ‚úÖ Prepare Output\n",
    "        result = {\n",
    "            # \"Page Title\": page_title,\n",
    "            # \"Images\": total_images,\n",
    "            # \"Buttons\": total_buttons,\n",
    "            # \"Links\": total_links,\n",
    "            \"UX Score\": ux_score,\n",
    "        }\n",
    "\n",
    "        # print(f\"üé® UX Score: {result['UX Score']}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        result = {\"error\": str(e)}\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        # print(f\"‚úÖ Finished Testing UX: {url}\\n\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1fd3d04-ac5c-4b70-973b-17823a2c5caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textstat in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (0.7.5)\n",
      "Requirement already satisfied: pyphen in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from textstat) (75.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from cmudict->textstat) (8.6.1)\n",
      "Requirement already satisfied: importlib-resources>=5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b477356b-8b1c-49a2-8c2a-578b306f57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import re\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3f04d5-3bdd-4f79-875d-97165d110438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Set up Selenium WebDriver\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "def get_page_source(url):\n",
    "    \"\"\"\n",
    "    Uses Selenium to get the full page source (including JavaScript-rendered content).\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The full HTML source code of the page.\n",
    "    \"\"\"\n",
    "    url = fix_url(url)  # Ensure correct format\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run without opening the browser\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for dynamic content to load\n",
    "        return driver.page_source  # Get full HTML\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching page source: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_text(url):\n",
    "    \"\"\"\n",
    "    Extracts visible text from a webpage using BeautifulSoup.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text content.\n",
    "    \"\"\"\n",
    "    html = get_page_source(url)\n",
    "    if not html:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove scripts, styles, and non-visible elements\n",
    "    for tag in soup([\"script\", \"style\", \"meta\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def analyze_content_quality(url):\n",
    "    \"\"\"\n",
    "    Analyzes a webpage's content based on readability, keyword density, text ratio, and heading structure.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The website URL.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing various content analysis metrics.\n",
    "    \"\"\"\n",
    "    # print(f\"\\nüöÄ Analyzing Content Quality: {url}\")\n",
    "    text = extract_text(url)\n",
    "\n",
    "    if not text:\n",
    "        print(\"‚ùå No text found on the page!\")\n",
    "        return {\"error\": \"No text content found\"}\n",
    "\n",
    "    # üìñ Readability Score\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "\n",
    "    # üìù Word Count\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Extract words\n",
    "    word_count = len(words)\n",
    "\n",
    "    # üîë Keyword Density (Top 5)\n",
    "    keyword_freq = Counter(words).most_common(5)\n",
    "\n",
    "    # üìä Text-to-HTML Ratio\n",
    "    html_content = get_page_source(url)\n",
    "    html_length = len(html_content)\n",
    "    text_length = len(text)\n",
    "    text_to_html_ratio = (text_length / html_length) * 100 if html_length > 0 else 0\n",
    "\n",
    "    # üî† Heading Structure\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    headings = {\n",
    "        \"h1\": len(soup.find_all(\"h1\")),\n",
    "        \"h2\": len(soup.find_all(\"h2\")),\n",
    "        \"h3\": len(soup.find_all(\"h3\")),\n",
    "    }\n",
    "\n",
    "    # ‚úÖ Prepare Output\n",
    "    result = {\n",
    "        \"Readability Score\": round(readability, 2),\n",
    "        \"Word Count\": word_count,\n",
    "        \"Keyword Density (Top 5)\": keyword_freq,\n",
    "        \"Text-to-HTML Ratio (%)\": round(text_to_html_ratio, 2),\n",
    "        \"Heading Structure\": headings\n",
    "    }\n",
    "\n",
    "    # ‚úÖ Print Results\n",
    "    for key, value in result.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9596f67-97c6-46fd-8d51-07d2d29e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from colorthief import ColorThief\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "import webcolors\n",
    "from urllib.parse import urljoin\n",
    "import colorsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c51c77-07a3-4fcb-b46a-92975a9bd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Path to ChromeDriver (Modify if needed)\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "def extract_hex_colors(url, top_n=10):\n",
    "    \"\"\"\n",
    "    Extracts top N most used hex color values from inline and external CSS using Selenium.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): Website URL to analyze.\n",
    "        top_n (int): Number of top colors to return.\n",
    "\n",
    "    Returns:\n",
    "        list: Top extracted hex colors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Setup Selenium WebDriver\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run in background\n",
    "        options.add_argument(\"--disable-gpu\")  \n",
    "        options.add_argument(\"--no-sandbox\")  \n",
    "        service = Service(CHROMEDRIVER_PATH)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        # ‚úÖ Open the webpage\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for full render\n",
    "        \n",
    "        # ‚úÖ Extract HTML source with rendered CSS\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        driver.quit()  # Close the browser\n",
    "\n",
    "        css_content = \"\"\n",
    "\n",
    "        # ‚úÖ Extract inline CSS\n",
    "        css_content += \" \".join(style.get_text() for style in soup.find_all(\"style\"))\n",
    "\n",
    "        # ‚úÖ Extract external CSS\n",
    "        for link in soup.find_all(\"link\", rel=\"stylesheet\"):\n",
    "            href = link.get(\"href\")\n",
    "            if href and href.startswith((\"http\", \"/\", \"./\")):\n",
    "                full_url = urljoin(url, href)\n",
    "                try:\n",
    "                    css_response = requests.get(full_url, timeout=5)\n",
    "                    if css_response.status_code == 200:\n",
    "                        css_content += css_response.text\n",
    "                except requests.RequestException:\n",
    "                    pass  # Ignore inaccessible stylesheets\n",
    "\n",
    "        # ‚úÖ Regex to find hex colors\n",
    "        color_pattern = r'#[0-9a-fA-F]{6}|#[0-9a-fA-F]{3}'\n",
    "        colors = re.findall(color_pattern, css_content)\n",
    "\n",
    "        # ‚úÖ Normalize short hex codes (e.g., #abc ‚Üí #aabbcc)\n",
    "        def normalize_hex(hex_color):\n",
    "            return (\n",
    "                f\"#{hex_color[1]*2}{hex_color[2]*2}{hex_color[3]*2}\"\n",
    "                if len(hex_color) == 4\n",
    "                else hex_color\n",
    "            )\n",
    "\n",
    "        colors = [normalize_hex(color) for color in colors]\n",
    "\n",
    "        # ‚úÖ Count occurrences & return top N colors\n",
    "        color_counts = Counter(colors)\n",
    "        top_colors = [color for color, _ in color_counts.most_common(top_n)]\n",
    "\n",
    "        print(f\"\\nüîπ Extracted Hex Colors (Top {top_n} Used):\", top_colors)\n",
    "        return top_colors if top_colors else [\"No Colors Found\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting colors: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ‚úÖ Function to calculate RGB distance\n",
    "def rgb_distance(color1, color2):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two RGB colors.\n",
    "\n",
    "    Parameters:\n",
    "        color1 (str): First hex color (e.g., \"#FF5733\").\n",
    "        color2 (str): Second hex color (e.g., \"#A2B3C4\").\n",
    "\n",
    "    Returns:\n",
    "        float: Distance between colors (lower = more similar).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r1, g1, b1 = [int(color1[i:i+2], 16) for i in (1, 3, 5)]\n",
    "        r2, g2, b2 = [int(color2[i:i+2], 16) for i in (1, 3, 5)]\n",
    "        return ((r1 - r2) ** 2 + (g1 - g2) ** 2 + (b1 - b2) ** 2) ** 0.5\n",
    "    except ValueError:\n",
    "        return float(\"inf\")  # Return a large distance for invalid hex values\n",
    "\n",
    "# ‚úÖ Function to rate UI/UX match based on color similarity\n",
    "def rate_color_match(extracted_colors, reference_colors):\n",
    "    \"\"\"\n",
    "    Rates how well extracted colors match a reference color palette.\n",
    "\n",
    "    Parameters:\n",
    "        extracted_colors (list): List of extracted hex colors.\n",
    "        reference_colors (list): List of predefined reference colors.\n",
    "\n",
    "    Returns:\n",
    "        float: UI/UX match rating (0-10).\n",
    "    \"\"\"\n",
    "    if not extracted_colors or extracted_colors == [\"No Colors Found\"]:\n",
    "        print(\"‚ùå No colors found, cannot rate UI/UX match.\")\n",
    "        return 0.0  # No colors found = 0 match\n",
    "\n",
    "    scores = []\n",
    "    for color in extracted_colors:\n",
    "        min_distance = min(rgb_distance(color, ref_color) for ref_color in reference_colors)\n",
    "        score = max(0, 10 - min_distance / 50)  # Lower distance = higher score\n",
    "        scores.append(score)\n",
    "\n",
    "    rating = round(sum(scores) / len(scores), 2) if scores else 0\n",
    "    return rating\n",
    "\n",
    "# ‚úÖ Define a reference color palette (Adjust based on industry standards)\n",
    "reference_colors = [\"#FF5733\", \"#A2B3C4\", \"#3E8E41\", \"#F1C40F\", \"#0A53BE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6394ef4-408e-4167-8a66-0d28226c9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "def check_mobile_friendliness(url):\n",
    "    \"\"\"Checks mobile friendliness of a website using Selenium.\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Setup WebDriver with Headless Mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")  \n",
    "        options.add_argument(\"--no-sandbox\")  \n",
    "        service = Service(CHROMEDRIVER_PATH)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # ‚úÖ Open the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for page to load\n",
    "\n",
    "        # print(\"\\nüì± Mobile-Friendliness Debug Info:\")\n",
    "\n",
    "        # ‚úÖ 1. Check for Viewport Meta Tag\n",
    "        viewport_tag = driver.find_elements(By.NAME, \"viewport\")\n",
    "        has_viewport = bool(viewport_tag)\n",
    "        # print(f\"‚úîÔ∏è Viewport Meta Tag: {has_viewport}\")\n",
    "\n",
    "        # ‚úÖ 2. Check for CSS Media Queries\n",
    "        has_media_queries = driver.execute_script(\"return window.matchMedia !== undefined\")\n",
    "        # print(f\"‚úîÔ∏è CSS Media Queries Found: {has_media_queries}\")\n",
    "\n",
    "        # ‚úÖ 3. Check if Body Width is Adaptable\n",
    "        body_width = driver.execute_script(\"return document.body.clientWidth\")\n",
    "        screen_width = driver.execute_script(\"return window.innerWidth\")\n",
    "        is_responsive = body_width <= screen_width\n",
    "        # print(f\"‚úîÔ∏è Responsive Layout Detected: {is_responsive} (Body: {body_width}px, Screen: {screen_width}px)\")\n",
    "\n",
    "        # ‚úÖ 4. Check for Horizontal Scrolling\n",
    "        horizontal_scroll = driver.execute_script(\"return document.body.scrollWidth > document.body.clientWidth\")\n",
    "        # print(f\"‚úîÔ∏è Horizontal Scroll Exists: {horizontal_scroll}\")\n",
    "\n",
    "        # ‚úÖ 5. Check for Touch-Friendly Buttons (Minimum 48px)\n",
    "        touch_elements = driver.execute_script(\"\"\"\n",
    "            return [...document.querySelectorAll('button, a, input, select, textarea')]\n",
    "                   .filter(el => el.getBoundingClientRect().height >= 48);\n",
    "        \"\"\")\n",
    "        has_touch_friendly_buttons = len(touch_elements) > 0\n",
    "        # print(f\"‚úîÔ∏è Touch-Friendly Elements Found: {has_touch_friendly_buttons}\")\n",
    "\n",
    "        # ‚úÖ Calculate Mobile-Friendliness Score (Out of 10)\n",
    "        mobile_friendly_score = 0\n",
    "        if has_viewport: mobile_friendly_score += 3\n",
    "        if has_media_queries: mobile_friendly_score += 2\n",
    "        if is_responsive: mobile_friendly_score += 2\n",
    "        if not horizontal_scroll: mobile_friendly_score += 2\n",
    "        if has_touch_friendly_buttons: mobile_friendly_score += 1\n",
    "\n",
    "        # ‚úÖ Ensure score is between 0 and 10\n",
    "        mobile_friendly_score = min(max(mobile_friendly_score, 0), 10)\n",
    "\n",
    "        # ‚úÖ Close WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "        # print(f\"\\nüì± Final Mobile Friendliness Score: {mobile_friendly_score}/10\")\n",
    "        return mobile_friendly_score\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking mobile friendliness: {e}\")\n",
    "        return None\n",
    "\n",
    "    # print(f\"üì± Mobile Friendliness Score: {result}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5601bbb0-1562-45c3-b22c-9acbfea545fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_page_load_speed(url):\n",
    "    \"\"\"Check the page load speed using Selenium's Performance Timing API.\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Setup WebDriver with Optimized Settings\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")  \n",
    "        options.add_argument(\"--no-sandbox\")  \n",
    "        options.add_argument(\"--disable-dev-shm-usage\")  # Helps in limited resource environments\n",
    "\n",
    "        service = Service(CHROMEDRIVER_PATH)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # ‚úÖ Start Timer & Open Website\n",
    "        start_time = time.time()\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow time for page to load\n",
    "\n",
    "        # ‚úÖ Extract Performance Timing Data\n",
    "        timing = driver.execute_script(\"return window.performance.timing\")\n",
    "        navigation_start = timing.get(\"navigationStart\", 0)\n",
    "        load_event_end = timing.get(\"loadEventEnd\", 0)\n",
    "\n",
    "        # ‚úÖ Close Browser Session\n",
    "        driver.quit()\n",
    "\n",
    "        # ‚úÖ Calculate Load Time (in Seconds)\n",
    "        if navigation_start == 0 or load_event_end == 0:\n",
    "            raise ValueError(\"Performance Timing API data is incomplete.\")\n",
    "        \n",
    "        page_load_time = (load_event_end - navigation_start) / 1000  # Convert to seconds\n",
    "\n",
    "        # ‚úÖ Score Calculation Based on Load Time\n",
    "        if page_load_time < 2:\n",
    "            score = 10\n",
    "        elif page_load_time < 5:\n",
    "            score = 7 + (2 - (page_load_time / 2))  # Smooth scaling\n",
    "        elif page_load_time < 10:\n",
    "            score = 4 + (5 - (page_load_time / 5))\n",
    "        else:\n",
    "            score = max(2, 10 - (page_load_time / 3))  # Ensure minimum score\n",
    "\n",
    "        # ‚úÖ Debugging Output\n",
    "        # print(\"\\nüöÄ Page Load Speed Analysis:\")\n",
    "\n",
    "        return {\n",
    "         \"page_load_score\": round(score, 2)  # ‚úÖ Removed incorrect syntax\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d766b948-7733-4c32-8bdb-1fee1eb06e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Path to ChromeDriver (Update as per your system)\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\Lenovo\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# ‚úÖ User-Agent to Mimic Real Browsing\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def check_broken_links(url):\n",
    "    \"\"\"Checks broken links on a website using Selenium & Requests.\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Setup WebDriver in Headless Mode\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(f\"user-agent={HEADERS['User-Agent']}\")\n",
    "        service = Service(CHROMEDRIVER_PATH)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # ‚úÖ Open Website\n",
    "        driver.get(url)\n",
    "        \n",
    "        # ‚úÖ Handle Access Denial\n",
    "        if \"403\" in driver.page_source or \"Access Denied\" in driver.page_source:\n",
    "            return {\"error\": \"This website blocks automated access. Skipping...\"}\n",
    "\n",
    "        # ‚úÖ Extract All Links\n",
    "        links = set(a.get_attribute(\"href\") for a in driver.find_elements(By.TAG_NAME, \"a\") if a.get_attribute(\"href\"))\n",
    "\n",
    "        driver.quit()  # Close browser\n",
    "\n",
    "        if not links:\n",
    "            return {\"error\": \"No links found on the webpage.\"}\n",
    "\n",
    "        # ‚úÖ Check Broken Links\n",
    "        broken_links = 0\n",
    "        total_links = len(links)\n",
    "\n",
    "        for link in links:\n",
    "            try:\n",
    "                response = requests.head(link, headers=HEADERS, allow_redirects=True, timeout=5)\n",
    "                if response.status_code >= 400:\n",
    "                    broken_links += 1\n",
    "            except requests.RequestException:\n",
    "                broken_links += 1  # Count timeouts/errors as broken links\n",
    "\n",
    "        # ‚úÖ Calculate Broken Link Score\n",
    "        broken_percentage = (broken_links / total_links) * 100 if total_links else 0\n",
    "        score = max(2, 10 - (broken_percentage / 5))  # Ensure a minimum score of 2\n",
    "        \n",
    "        return {\n",
    "            \"broken_link_score\": round(score, 2)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "735e9150-cfb7-481b-a3f6-ea7b07159d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import socket\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cee3ab-8749-402d-a1b4-92bdd0f1cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "class WebsiteAnalyzer:\n",
    "    def __init__(self, urls=None, file_path=None):\n",
    "        if urls:\n",
    "            if isinstance(urls, str):\n",
    "                self.urls = [urls]  # Convert single URL to list\n",
    "            else:\n",
    "                self.urls = urls  # List of URLs\n",
    "        elif file_path:\n",
    "            # Load URLs from the CSV file\n",
    "            self.urls = pd.read_csv(file_path)['Website URL'].tolist()\n",
    "        else:\n",
    "            self.urls = []\n",
    "\n",
    "    def analyze_single_url(self, url):\n",
    "        \"\"\"Analyze a single URL and return data as a dictionary.\"\"\"\n",
    "        start_time = time.time()  # Track time for each URL\n",
    "        try:\n",
    "            seo_data = fetch_seo_data(url)\n",
    "            if \"error\" in seo_data:\n",
    "                return None  # Skip URLs with errors\n",
    "            \n",
    "            website_type = detect_website_type(url, seo_data)\n",
    "            seo_score = calculate_seo_score(seo_data) if website_type == \"Company/Business Website\" else None\n",
    "            portfolio_score = analyze_portfolio(seo_data) if website_type == \"Personal Portfolio/Project\" else None\n",
    "            ui_analysis = analyze_ui(url)\n",
    "            ux_analysis = analyze_ux(url)\n",
    "            hex_colors = extract_hex_colors(url, top_n=10)\n",
    "            mobile_friendliness = check_mobile_friendliness(url)\n",
    "            page_speed = check_page_load_speed(url)\n",
    "            broken_links = check_broken_links(url)\n",
    "\n",
    "            # Store relevant data for DataFrame\n",
    "            data = {\n",
    "                \"Website URL\": url,\n",
    "                \"Website Type\": website_type,\n",
    "                \"SEO Score (%)\": seo_score if seo_score is not None else 0,\n",
    "                \"Portfolio Score (%)\": portfolio_score if portfolio_score is not None else 0,\n",
    "                \"UI Score (100)\": ui_analysis.get(\"UI Score\", 5),\n",
    "                \"UX Score (100)\": ux_analysis.get(\"UX Score\", 5),\n",
    "                \"Extracted Colors\": ', '.join(hex_colors) if isinstance(hex_colors, list) else \"N/A\",\n",
    "                \"Mobile Friendliness Score (10)\": mobile_friendliness if isinstance(mobile_friendliness, int) else mobile_friendliness.get(\"Mobile Friendliness Score\", 5),\n",
    "                \"Page Load Speed Score (10)\": page_speed if isinstance(page_speed, int) else page_speed.get(\"page_load_score\", 5),\n",
    "                \"Broken Links Score (10)\": broken_links if isinstance(broken_links, int) else broken_links.get(\"broken_link_score\", 5)\n",
    "            }\n",
    "\n",
    "            # ‚úÖ Calculate Overall Website Score\n",
    "            score_columns = [\"UI Score (100)\", \"UX Score (100)\", \"Mobile Friendliness Score (10)\", \"Page Load Speed Score (10)\", \"Broken Links Score (10)\"]\n",
    "            valid_scores = [data[col] for col in score_columns if data[col] is not None]\n",
    "\n",
    "            if valid_scores:\n",
    "                # Calculating the overall score by averaging the valid scores and multiplying by 10\n",
    "                data[\"Overall Score (100%)\"] = round((sum(valid_scores) / len(valid_scores)) * 10, 2)\n",
    "            else:\n",
    "                data[\"Overall Score (100%)\"] = 0  # Default to 0 if no valid scores\n",
    "\n",
    "            return data\n",
    "\n",
    "        except TimeoutError:\n",
    "            print(f\"‚ùå Timeout for {url}. Skipping this website.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error for {url}: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            # Check if the time taken exceeds 4 minutes (240 seconds)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > 240:  # 4 minutes timeout\n",
    "                print(f\"‚ùå {url} took too long to process ({elapsed_time:.2f} seconds). Skipping this website.\")\n",
    "                return None\n",
    "\n",
    "    def analyze_all(self):\n",
    "        \"\"\"Analyze all URLs concurrently and return a DataFrame.\"\"\"\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            # Use map to execute the analysis concurrently with a timeout of 240 seconds\n",
    "            results = list(executor.map(self.analyze_single_url, self.urls))\n",
    "        \n",
    "        # Filter out None values (websites that failed or were skipped)\n",
    "        results = [result for result in results if result]\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # ‚úÖ Save results to CSV\n",
    "        df.to_csv(\"website_analysis_results.csv\", index=False)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b90ec98-1918-4489-b6c6-c169bdc47bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Extracted Hex Colors (Top 10 Used): ['#1a73e8', '#ffffff', '#e8eaed', '#202124', '#5f6368', '#313131', '#4285f4', '#dadce0', '#3c4043', '#43a047']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website URL</th>\n",
       "      <th>Website Type</th>\n",
       "      <th>SEO Score (%)</th>\n",
       "      <th>Portfolio Score (%)</th>\n",
       "      <th>UI Score (100)</th>\n",
       "      <th>UX Score (100)</th>\n",
       "      <th>Extracted Colors</th>\n",
       "      <th>Mobile Friendliness Score (10)</th>\n",
       "      <th>Page Load Speed Score (10)</th>\n",
       "      <th>Broken Links Score (10)</th>\n",
       "      <th>Overall Score (100%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.kaggle.com</td>\n",
       "      <td>Company/Business Website</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>#1a73e8, #ffffff, #e8eaed, #202124, #5f6368, #...</td>\n",
       "      <td>10</td>\n",
       "      <td>7.68</td>\n",
       "      <td>5</td>\n",
       "      <td>445.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Website URL              Website Type  SEO Score (%)  \\\n",
       "0  https://www.kaggle.com  Company/Business Website           60.0   \n",
       "\n",
       "   Portfolio Score (%)  UI Score (100)  UX Score (100)  \\\n",
       "0                    0             100             100   \n",
       "\n",
       "                                    Extracted Colors  \\\n",
       "0  #1a73e8, #ffffff, #e8eaed, #202124, #5f6368, #...   \n",
       "\n",
       "   Mobile Friendliness Score (10)  Page Load Speed Score (10)  \\\n",
       "0                              10                        7.68   \n",
       "\n",
       "   Broken Links Score (10)  Overall Score (100%)  \n",
       "0                        5                445.36  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üéØ Usage Example:\n",
    "urls = [\"https://www.kaggle.com\"]\n",
    "analyzer = WebsiteAnalyzer(urls)\n",
    "df = analyzer.analyze_all()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cdaa7fe-7a70-4216-8972-1ef9bf7b360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Extracted Hex Colors (Top 10 Used): ['#ffffff', '#ddeebb', '#ededed', '#8F8F8F', '#d9d9d9', '#696969', '#323232', '#777777', '#DDDDDD', '#EEEEEE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website URL</th>\n",
       "      <th>Website Type</th>\n",
       "      <th>SEO Score (%)</th>\n",
       "      <th>Portfolio Score (%)</th>\n",
       "      <th>UI Score (100)</th>\n",
       "      <th>UX Score (100)</th>\n",
       "      <th>Extracted Colors</th>\n",
       "      <th>Mobile Friendliness Score (10)</th>\n",
       "      <th>Page Load Speed Score (10)</th>\n",
       "      <th>Broken Links Score (10)</th>\n",
       "      <th>Overall Score (100%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/</td>\n",
       "      <td>Uncategorized Website</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>#ffffff, #ddeebb, #ededed, #8F8F8F, #d9d9d9, #...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Website URL           Website Type  SEO Score (%)  \\\n",
       "0  https://www.zomato.com/  Uncategorized Website              0   \n",
       "\n",
       "   Portfolio Score (%)  UI Score (100)  UX Score (100)  \\\n",
       "0                    0             100             2.0   \n",
       "\n",
       "                                    Extracted Colors  \\\n",
       "0  #ffffff, #ddeebb, #ededed, #8F8F8F, #d9d9d9, #...   \n",
       "\n",
       "   Mobile Friendliness Score (10)  Page Load Speed Score (10)  \\\n",
       "0                               9                          10   \n",
       "\n",
       "   Broken Links Score (10)  Overall Score (100%)  \n",
       "0                        5                 252.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üéØ Usage Example:\n",
    "urls = [\"https://www.zomato.com/\"]\n",
    "analyzer = WebsiteAnalyzer(urls)\n",
    "df = analyzer.analyze_all()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbc864-1b67-46b6-8746-d14c72c3d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc85af8-492c-455b-a791-ccfc5949a6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
